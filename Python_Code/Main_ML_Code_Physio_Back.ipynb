{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main_ML_Code_Physio_Back.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdalrahman9/back_physio/blob/master/Python_Code/Main_ML_Code_Physio_Back.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxY1STu31lII"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnfEmnzw1kB_"
      },
      "source": [
        "from matplotlib import interactive, pyplot as plt\n",
        "import numpy as np\n",
        "import math #needed for definition of pi\n",
        "import pandas as pd\n",
        "import time, datetime, csv, signal\n",
        "import os\n",
        "import re\n",
        "from seglearn.transform import InterpLongToWide\n",
        "from seglearn.transform import FeatureRep, Segment, Interp\n",
        "from seglearn.pipe import Pype\n",
        "from bokeh.models import ColumnDataSource, Grid, LinearAxis, MultiLine, Plot, Range1d, LayoutDOM\n",
        "\n",
        "\n",
        "\n",
        "def signal_dataframe(folder):\n",
        "  items = folder.split('/')\n",
        "  session_name = items[1]+\"_\"+items[2]+\"_\"+items[3]+\"_\"+items[4]\n",
        "  '''###\n",
        "  MERGE CODE\n",
        "  '''###\n",
        "  for id_sensor , (sensor,sensor_name) in enumerate(zip([A,B,C,D,E,F,G,H],['A','B','C','D','E','F','G','H'])):\n",
        "    if 'reps' in items:\n",
        "        ending = \"_\"+items[6]+\".csv\"\n",
        "    else:\n",
        "        ending = \".csv\"\n",
        "\n",
        "\n",
        "    dfa = pd.read_csv(folder+session_name+'_'+sensor.replace(':','')+'_acc'+ending) #Reading the dataset in a dataframe using Pandas\n",
        "    dfg = pd.read_csv(folder+session_name+\"_\"+sensor.replace(':','')+\"_gyro\"+ending) #Reading the dataset in a dataframe using Pandas\n",
        "    dfm = pd.read_csv(folder+session_name+\"_\"+sensor.replace(':','')+\"_mag\"+ending) #Reading the dataset in a dataframe using Pandas\n",
        "    dfq = pd.read_csv(folder+session_name+\"_\"+sensor.replace(':','')+\"_quat\"+ending) #Reading the dataset in a dataframe using Pandas\n",
        "    dfp = pd.read_csv(folder+session_name+\"_\"+sensor.replace(':','')+\"_pres\"+ending) #Reading the dataset in a dataframe using Pandas\n",
        "\n",
        "    for signal_id , signal in enumerate([dfa,dfg,dfm,dfq,dfp]):\n",
        "      #Drop Time column\n",
        "      signal.drop(columns=['time (-07:00)'],inplace=True)\n",
        "\n",
        "      #Switch order of elapsed time and epoch\n",
        "      cols  = list(signal.columns)\n",
        "      cols[0], cols[1] = cols[1] , cols[0]\n",
        "      signal = signal[cols]\n",
        "\n",
        "      #Interpolate the data to have all signals to 25Hz\n",
        "      clf = Pype([('interp', Interp(1. / 25., categorical_target=True))])\n",
        "      # print(signal[cols].columns.values[1:])\n",
        "      # test = ([dfa.to_numpy(),dfg.to_numpy(),dfm.to_numpy(),dfq.to_numpy(),dfp.to_numpy()])\n",
        "      signal_arr, _ = clf.fit_transform([signal.to_numpy()],[0])\n",
        "      signal_arr = pd.DataFrame(data=signal_arr[0],columns=signal[cols].columns.values[1:])\n",
        "      if signal_id == 0:\n",
        "        dfa =  signal_arr\n",
        "      elif signal_id == 1:\n",
        "        dfg =  signal_arr\n",
        "      elif signal_id == 2:\n",
        "        dfm =  signal_arr\n",
        "      elif signal_id == 3:\n",
        "        dfq =  signal_arr\n",
        "      else:\n",
        "        dfp =  signal_arr\n",
        "\n",
        "    #Synchronizing and merging the signals together\n",
        "    test1a = pd.merge_asof(dfa, dfg, on='epoch (ms)')\n",
        "    test2a = pd.merge_asof(test1a, dfm, on='epoch (ms)')\n",
        "    test3a = pd.merge_asof(test2a, dfq, on='epoch (ms)')\n",
        "    test4a = pd.merge_asof(test3a, dfp, on='epoch (ms)')\n",
        "\n",
        "    df = test4a\n",
        "\n",
        "    #Add sensor name to each of its corresponding columns\n",
        "    for signal in sensor_type_col_labels:\n",
        "      for axis in signal:\n",
        "        df.rename(columns = {axis:axis + \" - \"+ sensor_name}, inplace = True)\n",
        "        \n",
        "    if id_sensor == 0:\n",
        "      full_df = df.copy()\n",
        "    else:\n",
        "      full_df = pd.merge_asof(full_df, df, on='epoch (ms)')\n",
        "\n",
        "  # Remove any NA rows and reset index before returning array\n",
        "  full_df.dropna(inplace=True)\n",
        "  full_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  #Taring epochs to first epoch entry (eg. Epoch[0] = 0 ms) \n",
        "  full_df['epoch (ms)'] = ((full_df['epoch (ms)']).sub(full_df['epoch (ms)'][0])).div(1000)\n",
        "\n",
        "  #Convert dataframe to ndarray and return it\n",
        "  return full_df.to_numpy()\n",
        "\n",
        "\n",
        "################################### MAIN CODE ######################################\n",
        "#1 Laying Down\n",
        "#2 Sustained Extension\n",
        "#4 Sagittal Extension\n",
        "#5 Extension in Standing\n",
        "#6 Flexion in Lying\n",
        "#7 Flexion in Sitting\n",
        "#8 Flexion in Standing\n",
        "#9 Side Glide in Laying\n",
        "#10 Side Glide in Standing\n",
        "#11 Rotation Flexion Knees Together\n",
        "#12 Rotation Flexion One leg stretch\n",
        "#13 Other\n",
        "\n",
        "###Define Global variables and arrays###\n",
        "\n",
        "#Sensor MAC\n",
        "A = \"C7:E1:38:1F:C0:DE\"\n",
        "B = \"F4:04:52:A2:CB:59\"\n",
        "C = \"E3:62:1F:8B:81:B7\"\n",
        "D = \"E8:9C:A5:A3:8A:60\"\n",
        "E = \"F9:0E:1C:DA:D4:1D\"\n",
        "F = \"CD:A5:4D:78:A1:B4\"\n",
        "G = \"EF:AA:47:DC:45:44\"\n",
        "H = \"CD:78:F1:6B:D8:67\"\n",
        "\n",
        "#Channel names\n",
        "acc_col_label = ['X-Axis (g)','Y-Axis (g)','Z-Axis (g)']\n",
        "gyro_col_label =['X-Axis (deg/s)','Y-Axis (deg/s)','Z-Axis (deg/s)']\n",
        "mag_col_label = ['X-Axis (T)','Y-Axis (T)','Z-Axis (T)']\n",
        "pres_col_label = ['Pressure (Pa)']\n",
        "quat_col_label = ['W-Axis','X-Axis (i)','Y-Axis (j)','Z-Axis (k)'] #should be i j k\n",
        "sensor_type_col_labels = [acc_col_label,gyro_col_label,mag_col_label,quat_col_label,pres_col_label]\n",
        "\n",
        "#Selecting the Participant Data that wants to be analyzed \n",
        "subj_names = ['p0','p1','p2','p3','p4','p5','p6','p7','p8','p9','p10','p11','p12','p13','p14','p15','p16','p17','p18']\n",
        "\n",
        "#Defining the data dict that will house the imported data\n",
        "data = {'X': [], 'Subject': [], 'Exercise': [], 'Side': [], 'Rep': [], 'Sensor': [], 'Signal' : [], 'Axis': [], 'Y': []}\n",
        "\n",
        "#Specify data folder\n",
        "directory = \"Data/\"\n",
        "\n",
        "#Important the relevent data files into the data dict\n",
        "for subj in subj_names:\t#Enter Subject Folder\n",
        "    folder = directory + subj + \"/\"   #update folder string with new directory\n",
        "\n",
        "    #Loop only with newest session with \"[max(os.listdir(folder))]\"\n",
        "    for session in [s for s in [max(os.listdir(folder))] if os.path.isdir(folder+s)]:\t#Enter Session Folder\n",
        "        folder = directory + subj + \"/\" + session + \"/\"   #update folder string with new directory\n",
        "\n",
        "        #Loop through each exercise folder except for nonrelevent folders (eg. tar & test folders)\n",
        "        #Change which folders you want to exclude based on which data you want (Desire all data or only posture or exercise)\n",
        "        for exercise in [e for e in os.listdir(folder) if os.path.isdir(folder+e) and e!='tar' and e!='test' and e!='offset' #and e!='pg' and e!='pfg' and e!='pb']: \n",
        "                         and e!='e1' and e!='e2' and e!='e3' and e!='e4' and e!='e5' and e!='e6' and e!='e7' and e!='random']:\t#Enter Exercise Folder\n",
        "            folder = directory + subj + \"/\" + session + \"/\" + exercise + \"/\"    #update folder string with new directory\n",
        "\n",
        "            #Extract all Round Folders and select the newest round\n",
        "            round_folders = [r for r in os.listdir(folder) if os.path.isdir(folder+r)]\n",
        "            #First check 2 sided exercises\n",
        "            if exercise in ['e3','e4','e7']:  #Where there are 2 sides (L/R)\n",
        "                left = [place for place in round_folders if 'l' in place]\n",
        "                right = [right for right in round_folders if right not in left]\n",
        "                if not left:\n",
        "                    round_folders = [max(right)]\n",
        "                elif not right:\n",
        "                    round_folders = [max(left)]\n",
        "                else:\n",
        "                    round_folders = [max(left),max(right)]\n",
        "            #Next check One sided exercises\n",
        "            else:# One sided Exercises\n",
        "                round_folders = [max(round_folders)]\n",
        "\n",
        "            #Now enter Round folder\n",
        "            for rounds in round_folders:\t#Enter Round Folder\n",
        "                folder = directory + subj + \"/\" + session + \"/\" + exercise + \"/\" + rounds + \"/\"   #update folder string with new directory\n",
        "                \n",
        "                #Check if there is a reps folder\n",
        "                if os.path.isdir(folder+\"reps\"): #Check if there is a reps folder\n",
        "                    folder = directory + subj + \"/\" + session + \"/\" + exercise + \"/\" + rounds + \"/reps/\"\n",
        "                    for reps in [r for r in os.listdir(folder) if os.path.isdir(folder+r)]:\n",
        "                        folder = directory + subj + \"/\" + session + \"/\" + exercise + \"/\" + rounds + \"/reps/\" + reps + \"/\"\n",
        "                        \n",
        "                        #Extract data from files and label the entry with the correspoding data & tags\n",
        "                        all_signals = signal_dataframe(folder)\n",
        "                        data['X'].append(all_signals)\n",
        "                        data['Subject'].append((re.findall(r\"\\d+\", subj))[0])\n",
        "                        data['Exercise'].append(exercise)\n",
        "                        data['Rep'].append(int(reps)+1)\n",
        "\n",
        "                        #If exercise has 2 sides, put the applicable side\n",
        "                        if 'l' in rounds:\n",
        "                            data['Side'].append('Left')\n",
        "                        elif rounds.count('r') == 2:\n",
        "                            data['Side'].append('Right')\n",
        "                        else:\n",
        "                            data['Side'].append('NA')\n",
        "                        print(\"Saved \"+folder)\n",
        "\n",
        "                #For exercises with no reps (ex. pg & e1)\n",
        "                else:\t#No Reps Folder Found\n",
        "                      #Extract data from files and label the entry with the correspoding data & tags\n",
        "                      all_signals = signal_dataframe(folder)\n",
        "                      data['X'].append(all_signals)\n",
        "                      data['Subject'].append((re.findall(r\"\\d+\", subj))[0])\n",
        "                      data['Exercise'].append(exercise)\n",
        "                      data['Rep'].append(1)\n",
        "                      \n",
        "                      #If exercise has 2 sides, put the applicable side\n",
        "                      if 'l' in rounds:\n",
        "                          data['Side'].append('Left')\n",
        "                      elif rounds.count('r') == 2:\n",
        "                          data['Side'].append('Right')\n",
        "                      else:\n",
        "                          data['Side'].append('NA')\n",
        "                      print(\"Saved \"+folder)\n",
        "\n",
        "#Save all the labels into one key for easier manipulation later on\n",
        "data['Y'] = np.column_stack([data['Subject'], data['Exercise'], data['Side'], data['Rep']])\n",
        "\n",
        "#Output the sizes of the important data\n",
        "print(\"Finished Saving Files\")\n",
        "\n",
        "print(\"# X \" + str(len(data['X'])))\n",
        "print(\"# Side \" + str(len(data['Side'])))\n",
        "print(\"# Subject \" + str(len(data['Subject'])))\n",
        "print(\"# Exercise \" + str(len(data['Exercise'])))\n",
        "print(\"# Rep \" + str(len(data['Rep'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyVoho87k_mP"
      },
      "source": [
        "#Save data file in directory for simpler usage later on\n",
        "np.save('posture_data.npy', data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Grk5p6mEYt"
      },
      "source": [
        "#Load the data file if it already exists \n",
        "data = (np.load('posture_data.npy',allow_pickle=True))[()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYpOOndDImYP"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jboJHkyUOUmZ"
      },
      "source": [
        "import seglearn as sgl\n",
        "\n",
        "#Import all features from seglearn library\n",
        "d = sgl.feature_functions.all_features()\n",
        "\n",
        "#Delete features that are not applicable or don't work\n",
        "del d['hmean']  #all elements have to be positive to calc the harmonic mean -- condition not satisfied\n",
        "del d['gmean']  #all elements have to be positive to calc the harmonic mean -- condition not satisfied\n",
        "del d['emg_var']  #EMG var not applicable for data set/problem (IMU data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLVjKm1LlvgH"
      },
      "source": [
        "#These two Functions should work but I get an error of: \"dividing my zero\" .. trying to fix\n",
        "del d['corr'] #func not working....\n",
        "del d['hist4']  #func not working...."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIWNyLDv6Mv0"
      },
      "source": [
        "def corr2(X):\n",
        "    \"\"\" computes correlations between all variable pairs in a segmented time series\n",
        "\n",
        "    .. note:: this feature is expensive to compute with the current implementation, and cannot be\n",
        "    used with univariate time series\n",
        "    \"\"\"\n",
        "    # print(X.shape)\n",
        "    X = np.atleast_3d(X)\n",
        "    # print(X.shape)\n",
        "    N = X.shape[0]\n",
        "    D = X.shape[2]\n",
        "\n",
        "    if D == 1:\n",
        "        return np.zeros(N, dtype=np.float)\n",
        "\n",
        "    trii = np.triu_indices(D, k=1) \n",
        "    DD = len(trii[0])\n",
        "    r = np.zeros((N, DD))\n",
        "    for i in np.arange(N):\n",
        "        rmat = np.corrcoef(X[i].transpose())  # get the ith window from each signal, result will be DxD\n",
        "        r[i] = rmat[trii]\n",
        "    return r\n",
        "\n",
        "#Replace old corr function with this altered version\n",
        "d['corr'] = corr2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djM_iccl1uM7"
      },
      "source": [
        "## Pipeline Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVOXpOHqvnwg"
      },
      "source": [
        "Creation of sensor/signal selection class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_S4_GA9uwNo"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "#Class for selecting specific Sensors and Signals from the original data\n",
        "class Sensor_Select(BaseEstimator,TransformerMixin):\n",
        "   'Common base class for sensor/signal Selection'\n",
        "   #Declare Sensor Indices within the 113 channel columns (originally 112 but epoch column was added thus 113)\n",
        "   A = 1\n",
        "   B = 15\n",
        "   C = 29\n",
        "   D = 43\n",
        "   E = 57\n",
        "   F = 71\n",
        "   G = 85\n",
        "   H = 99\n",
        "\n",
        "   #Declare the relative Signal indices within each sensor segment -> 14 signals within each sensor \n",
        "   acc = [0 , 1 , 2]\n",
        "   gyro = [3 , 4 , 5]\n",
        "   mag = [6, 7, 8]\n",
        "   quat = [9, 10, 11, 12]\n",
        "   pres = [13]\n",
        "\n",
        "   def __init__(self, sensors=[], signals=[]):\n",
        "     #Init the sensor, signal arrays\n",
        "     print(\"Entered Init\")\n",
        "     self.sensors = sensors\n",
        "     self.signals = signals\n",
        "     \n",
        "     #init an empty list to hold the respective indices for the selected sensors and signals\n",
        "     self.sensors_int = []\n",
        "     self.signals_int = []\n",
        "   \n",
        "   def fit(self,X,y=None):\n",
        "     print(\"Entered Fit\")\n",
        "     return self\n",
        "\n",
        "   def transform(self,X,y=None):\n",
        "     print(\"Entered transform\")\n",
        "\n",
        "     #Loop through the desired sensors and extract their respective indices within the 113 columns\n",
        "     for dev , indices in zip(['A','B','C','D','E','F','G','H'],[[self.A],[self.B],[self.C],[self.D],[self.E],[self.F],[self.G],[self.H]]):\n",
        "       if dev in self.sensors: self.sensors_int.extend(indices)\n",
        "\n",
        "     #Now loop through the desired signals and extract their respective indices within each sensor segment\n",
        "     for ch , relative_indices in zip(['acc','gyro','mag','quat','pres'], [self.acc,self.gyro,self.mag,self.quat,self.pres]):\n",
        "       if ch in self.signals: self.signals_int.extend(relative_indices)\n",
        "       \n",
        "     #Now Combine both sensor and signal indices to extract all the desired signals from each sensor\n",
        "     #I included column '0' by default within the list because it corresponds to the 'epoch' column and must\n",
        "     #be included no matter what sensors/signals are desired\n",
        "     Filtered_Indices = [0] + [i+ j for i in self.sensors_int for j in self.signals_int]\n",
        "          \n",
        "     #Now insert the desired indices (ie. columns/signals) within the main data and retrieve the desired sensors along with their signals\n",
        "     filtered_data = [entry[:,Filtered_Indices] for entry in X]\n",
        "\n",
        "     #Return the filtered data\n",
        "     return filtered_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps143uLovtOM"
      },
      "source": [
        "Running Pipe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv_ZaMOX1sB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96fed27-5856-44cc-870b-e294014e75bb"
      },
      "source": [
        "import seglearn as sgl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Create the pype with all the main processing elements\n",
        "clf = sgl.Pype([('sensors', Sensor_Select(['A','B','C','D','E','F','G','H'],['acc','gyro','mag','quat','pres'])),\n",
        "                ('interp', Interp(1. / 25., categorical_target=True)),\n",
        "                ('segment', sgl.Segment(width=25,overlap=0.3)),\n",
        "                ('ftr', sgl.FeatureRep(features=d)),\n",
        "                ('scaler', StandardScaler())])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/abdallah/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass memory=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
            "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGKGbIuppnAq",
        "outputId": "22817cb4-95ee-48d1-e88e-ad5fecbb6c1d"
      },
      "source": [
        "#Run the data through the pipe\n",
        "X, y = clf.fit_transform(data['X'], data['Y'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/abdallah/.local/lib/python3.8/site-packages/seglearn/transform.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  Xt = np.array([sliding_tensor(Xt[i], self.width, self._step, self.order)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}